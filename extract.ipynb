{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Extract - Data Loading\n",
        "\n",
        "This notebook handles the extraction of raw data from source files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "np.random.seed(34)\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define column names\n",
        "index_names = ['unit_number', 'time_cycles']\n",
        "setting_names = ['setting_1', 'setting_2', 'setting_3']\n",
        "sensor_names = ['s_{}'.format(i+1) for i in range(0,21)]\n",
        "col_names = index_names + setting_names + sensor_names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract raw data from ALL source files (FD001-FD004, train and test)\n",
        "# Combine all files into a single unified dataset\n",
        "import glob\n",
        "\n",
        "# Find all train and test files\n",
        "train_files = sorted(glob.glob('train_FD*.txt'))\n",
        "test_files = sorted(glob.glob('test_FD*.txt'))\n",
        "rul_files = sorted(glob.glob('RUL_FD*.txt'))\n",
        "\n",
        "print(f\"Found {len(train_files)} train files: {[f.split('/')[-1] for f in train_files]}\")\n",
        "print(f\"Found {len(test_files)} test files: {[f.split('/')[-1] for f in test_files]}\")\n",
        "print(f\"Found {len(rul_files)} RUL files: {[f.split('/')[-1] for f in rul_files]}\")\n",
        "\n",
        "# Load and combine all train files\n",
        "train_dataframes = []\n",
        "for file in train_files:\n",
        "    dataset_id = file.replace('train_', '').replace('.txt', '')  # e.g., 'FD001'\n",
        "    df = pd.read_csv(file, sep='\\s+', header=None, index_col=False, names=col_names)\n",
        "    df['dataset_id'] = dataset_id\n",
        "    df['source_file'] = 'train'\n",
        "    train_dataframes.append(df)\n",
        "    print(f\"  Loaded {file}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "\n",
        "# Load and combine all test files\n",
        "test_dataframes = []\n",
        "for file in test_files:\n",
        "    dataset_id = file.replace('test_', '').replace('.txt', '')  # e.g., 'FD001'\n",
        "    df = pd.read_csv(file, sep='\\s+', header=None, index_col=False, names=col_names)\n",
        "    df['dataset_id'] = dataset_id\n",
        "    df['source_file'] = 'test'\n",
        "    test_dataframes.append(df)\n",
        "    print(f\"  Loaded {file}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "\n",
        "# Combine all data into single dataframe\n",
        "all_data = pd.concat(train_dataframes + test_dataframes, ignore_index=True)\n",
        "print(f\"\\nâœ“ Combined dataset: {all_data.shape[0]} rows, {all_data.shape[1]} columns\")\n",
        "print(f\"  Unique engines: {all_data['unit_number'].nunique()}\")\n",
        "print(f\"  Datasets: {sorted(all_data['dataset_id'].unique())}\")\n",
        "print(f\"  Source files: {sorted(all_data['source_file'].unique())}\")\n",
        "\n",
        "# Load RUL files for reference (optional, for validation purposes)\n",
        "rul_dataframes = []\n",
        "for file in rul_files:\n",
        "    dataset_id = file.replace('RUL_', '').replace('.txt', '')\n",
        "    df = pd.read_csv(file, sep='\\s+', header=None, index_col=False, names=['RUL'])\n",
        "    df['dataset_id'] = dataset_id\n",
        "    rul_dataframes.append(df)\n",
        "\n",
        "if rul_dataframes:\n",
        "    all_rul = pd.concat(rul_dataframes, ignore_index=True)\n",
        "    print(f\"  RUL reference data: {all_rul.shape[0]} rows\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create unified dataset for processing\n",
        "data = all_data.copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of the train dataset :  (20631, 26)\n",
            "Shape of the validation dataset :  (13096, 26)\n",
            "Shape of the RUL validation dataset :  (100, 1)\n"
          ]
        }
      ],
      "source": [
        "# Verify extraction\n",
        "print('Shape of the combined dataset:', data.shape)\n",
        "print('\\nBreakdown by dataset:')\n",
        "print(data.groupby('dataset_id').size())\n",
        "print('\\nBreakdown by source file:')\n",
        "print(data.groupby('source_file').size())\n",
        "print('\\nSample data:')\n",
        "print(data.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save extracted data for next stage (Transform)\n",
        "import os\n",
        "data.to_csv('data_extracted.csv', index=False)\n",
        "if rul_dataframes:\n",
        "    all_rul.to_csv('rul_extracted.csv', index=False)\n",
        "    print(\"Extracted data and RUL reference saved for transformation stage\")\n",
        "else:\n",
        "    print(\"Extracted data saved for transformation stage\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
